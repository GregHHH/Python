{"cells":[{"cell_type":"markdown","metadata":{"id":"_x3c-ePP0jZG"},"source":["# **TP Cours Textmining** \n","#### *Etude port√©e sur les tweets de deux politiques fran√ßais*\n","\n","*05-02-2021*\n","\n","Les objectifs de ce TP sont : \n","- Travailler sur du texte fran√ßais\n","- Analyser les donn√©es \n","- Reprendre les acquis d√©velopp√©s dans les TP pr√©c√©dents\n","- D√©couvrir de nouveaux outils : scattertext\n","- Pr√©dire qui a post√© un tweet "]},{"cell_type":"markdown","metadata":{"id":"RII9vVrm1bLG"},"source":["## **1. Installation des packages**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29105,"status":"ok","timestamp":1644497578663,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"WptkukJYK4ur","outputId":"010523f8-29a0-48a2-ff0c-7b1ef3c3ad48"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting scattertext\n","  Downloading scattertext-0.1.5-py3-none-any.whl (7.3 MB)\n","     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.3 MB 4.3 MB/s            \n","\u001b[?25hRequirement already satisfied: pandas in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scattertext) (1.2.4)\n","Requirement already satisfied: scikit-learn in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scattertext) (0.24.1)\n","Requirement already satisfied: numpy in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scattertext) (1.20.1)\n","Collecting flashtext\n","  Downloading flashtext-2.7.tar.gz (14 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: six in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scattertext) (1.15.0)\n","Requirement already satisfied: statsmodels in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scattertext) (0.13.1)\n","Requirement already satisfied: scipy in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scattertext) (1.6.2)\n","Collecting gensim>=4.0.0\n","  Downloading gensim-4.1.2-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n","     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24.0 MB 7.3 MB/s             \n","\u001b[?25hRequirement already satisfied: mock in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scattertext) (4.0.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from gensim>=4.0.0->scattertext) (5.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from pandas->scattertext) (2.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from pandas->scattertext) (2021.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->scattertext) (2.1.0)\n","Requirement already satisfied: joblib>=0.11 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->scattertext) (1.0.1)\n","Requirement already satisfied: patsy>=0.5.2 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from statsmodels->scattertext) (0.5.2)\n","Building wheels for collected packages: flashtext\n","  Building wheel for flashtext (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=787abd3c1097f668f15953a507739eb9b4140ed3fe3390973f846c4a1bb047af\n","  Stored in directory: /Users/greg/Library/Caches/pip/wheels/8d/62/8b/71813348245ae1bcbae179193bbc72db819e8057e89298a6ac\n","Successfully built flashtext\n","Installing collected packages: gensim, flashtext, scattertext\n","Successfully installed flashtext-2.7 gensim-4.1.2 scattertext-0.1.5\n","\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n","You should consider upgrading via the '/Users/greg/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","Requirement already satisfied: spacy in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (3.0.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.9.0)\n","Requirement already satisfied: setuptools in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (60.5.0)\n","Requirement already satisfied: pathy>=0.3.5 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.25.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.2)\n","Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.7.4)\n","Requirement already satisfied: jinja2 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.13)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.6)\n","Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.3.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.59.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.6)\n","Requirement already satisfied: numpy>=1.15.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.20.1)\n","Requirement already satisfied: packaging>=20.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n","Requirement already satisfied: pyparsing>=2.0.2 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n","\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n","You should consider upgrading via the '/Users/greg/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","Requirement already satisfied: nltk in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (3.6.1)\n","Requirement already satisfied: click in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n","Requirement already satisfied: regex in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n","Requirement already satisfied: joblib in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n","Requirement already satisfied: tqdm in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n","\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n","You should consider upgrading via the '/Users/greg/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","Requirement already satisfied: termcolor in /Users/greg/opt/anaconda3/lib/python3.8/site-packages (1.1.0)\n","\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n","You should consider upgrading via the '/Users/greg/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["# !pip install scattertext\n","# !pip install spacy\n","# !pip install nltk\n","# !pip install termcolor"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34370,"status":"ok","timestamp":1644497613002,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"63W-DuD0keX3","outputId":"97c5e3d6-33d4-4b4a-db3e-6a3c05bbeddb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting fr_core_news_md==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-2.2.5/fr_core_news_md-2.2.5.tar.gz (88.6 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88.6 MB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_md==2.2.5) (2.2.4)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (0.9.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.19.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (4.62.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.23.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (7.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (2.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.6)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (4.10.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.10.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_md==2.2.5) (1.24.3)\n","Building wheels for collected packages: fr-core-news-md\n","  Building wheel for fr-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fr-core-news-md: filename=fr_core_news_md-2.2.5-py3-none-any.whl size=90338488 sha256=9062b02022a5efc7bb41052b6b2a9e1753867d3ef32b529ca9e5440633ab5fd8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ooskq1qu/wheels/2e/26/ff/ce93eb966e7176ebe81e6c98209582e13e108cdd2d6d636df0\n","Successfully built fr-core-news-md\n","Installing collected packages: fr-core-news-md\n","Successfully installed fr-core-news-md-2.2.5\n","\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('fr_core_news_md')\n"]}],"source":["#!python -m spacy download fr_core_news_md"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1644498177032,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"G1d_nO78LIy9"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from time import time\n","import datetime\n","\n","# Modules de traitement du texte\n","import spacy\n","import fr_core_news_md\n","import nltk\n","import re\n","from termcolor import colored\n","\n","# Modules pour le wordcloud\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","# Module pour scattertext\n","import scattertext as st\n","\n","# Modules de mod√©lisation\n","from sklearn.utils.fixes import loguniform\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":270,"status":"ok","timestamp":1644498135859,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"ZdeI55hXMWYA"},"outputs":[],"source":["# chemin o√π se trouve le jeu de donn√©es (tweets_politics_2022.csv)\n","PATH_DATA = '/data' #TODO mettre le chemin pour trouver le fichier # "]},{"cell_type":"markdown","metadata":{"id":"S5yIKyNBL8sG"},"source":["## **2. Prise en main de la base de donn√©es**\n","\n","Les donn√©es ont √©t√© extraites via l'API tweepy dans un autre notebook. \\\n","Les tweets de certains candidats √† l'√©lection pr√©sidentiels ont √©t√© r√©cup√©r√©s. \n","\n","Regardez les variables √† disposition, quelques comptages, s'il y a des donn√©es manquantes, quelques graphiques (?), la sp√©cificit√© des tweets, etc. "]},{"cell_type":"markdown","metadata":{"id":"1y8B9ztR5Iee"},"source":["#### Import des donn√©es"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":244,"status":"ok","timestamp":1644507496865,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"Fl_jd11jLI4R"},"outputs":[],"source":["df_tweets = pd.read_csv(f'tweets_politics_2022.csv')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1644507497282,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"wb6War732Z4H","outputId":"a91b8dd0-6a8f-4862-d9be-38d3310fdacd"},"outputs":[{"data":{"text/plain":["(18430, 6)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_tweets.shape"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1644507497794,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"jmNq0KrHOtd-","outputId":"c770e0a5-7eac-4391-b006-f18b659837fb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>created_at</th>\n","      <th>favorite_count</th>\n","      <th>retweet_count</th>\n","      <th>text</th>\n","      <th>user_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1487771447992926210</td>\n","      <td>2022-01-30 12:55:29</td>\n","      <td>436.0</td>\n","      <td>193.0</td>\n","      <td>Redonner du sens √† la gauche : se rassembler a...</td>\n","      <td>JeanLuc_Melenchon</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1487400906517848070</td>\n","      <td>2022-01-29 12:23:05</td>\n","      <td>2350.0</td>\n","      <td>1027.0</td>\n","      <td>L‚Äôinscription sur les listes √©lectorales c‚Äôest...</td>\n","      <td>JeanLuc_Melenchon</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1487117751084855300</td>\n","      <td>2022-01-28 17:37:55</td>\n","      <td>1145.0</td>\n","      <td>480.0</td>\n","      <td>üî¥ Rendez-vous ce dimanche 30 janvier √† 20h55 s...</td>\n","      <td>JeanLuc_Melenchon</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1487104482336989191</td>\n","      <td>2022-01-28 16:45:12</td>\n","      <td>1164.0</td>\n","      <td>437.0</td>\n","      <td>3 solutions pour financer la retraite √† 60 ans...</td>\n","      <td>JeanLuc_Melenchon</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1487080530558476288</td>\n","      <td>2022-01-28 15:10:01</td>\n","      <td>1591.0</td>\n","      <td>551.0</td>\n","      <td>La M√©diterran√©e est le plus grand cimeti√®re du...</td>\n","      <td>JeanLuc_Melenchon</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    id           created_at  favorite_count  retweet_count  \\\n","0  1487771447992926210  2022-01-30 12:55:29           436.0          193.0   \n","1  1487400906517848070  2022-01-29 12:23:05          2350.0         1027.0   \n","2  1487117751084855300  2022-01-28 17:37:55          1145.0          480.0   \n","3  1487104482336989191  2022-01-28 16:45:12          1164.0          437.0   \n","4  1487080530558476288  2022-01-28 15:10:01          1591.0          551.0   \n","\n","                                                text            user_id  \n","0  Redonner du sens √† la gauche : se rassembler a...  JeanLuc_Melenchon  \n","1  L‚Äôinscription sur les listes √©lectorales c‚Äôest...  JeanLuc_Melenchon  \n","2  üî¥ Rendez-vous ce dimanche 30 janvier √† 20h55 s...  JeanLuc_Melenchon  \n","3  3 solutions pour financer la retraite √† 60 ans...  JeanLuc_Melenchon  \n","4  La M√©diterran√©e est le plus grand cimeti√®re du...  JeanLuc_Melenchon  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df_tweets.head()"]},{"cell_type":"markdown","metadata":{"id":"BlANmcVAPh3L"},"source":["####  Quelques comptages / graphiques"]},{"cell_type":"markdown","metadata":{"id":"k_J_CgyXSpUU"},"source":["##### Indicateurs simples sur les variables : \n","- Y a't'il des donn√©es manquantes ? \n","- combien de tweets de chaque candidat ? \n","- dates minimales / maximales des tweets\n","- Distribution des favoris et des retweets de chaque candidat"]},{"cell_type":"markdown","metadata":{"id":"Qd-6BM7i6ZvW"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> Utiliser .isnull() </li>\n","    <li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html\" >pd.value_counts()</a> pour compter le nombre de modalit√©s d'une variable cat√©gorielle </li>\n","    <li> Convertir la date au bon format avec pd.to_datetime() </li>\n","    <li> Vous pouvez utiliser la fonction groupby et describe() </li>\n","\n","</ul>\n","</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rpd04jMO9h1E"},"outputs":[],"source":["# Impl√©mentez la fonction en changeant le None\n","def check_missing_values(df):\n","  #TODO\n","  return None\n","\n","check_missing_values(df_tweets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndZ68VlXLI8M"},"outputs":[],"source":["# Combien de tweets dans la base de donn√©es pour chacun des candidats ? \n","#TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mpm5HJsSTmD"},"outputs":[],"source":["# A quelles dates ont √©t√© envoy√©s les premiers / derniers tweets des candidats ? \n","#TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kx4UvxNc81Js"},"outputs":[],"source":["# Quelle est la distribution des favoris et retweets des candidats  ?\n","#TODO"]},{"cell_type":"markdown","metadata":{"id":"zp38kr7YTIyP"},"source":["##### R√©partition du nombre de retweets / favoris dans le temps "]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":528,"status":"ok","timestamp":1644507501629,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"wUzv4R0HOF2p"},"outputs":[],"source":["def visualize_count_favorites(df, userID) : \n","  \n","  ''' Cette fonction permet de visualiser le nombre de favoris et de retweets \n","  sur toute la p√©riode pour un user_id donn√© '''\n","\n","  df_temp = df.loc[df[\"user_id\"] == userID]\n","  ylabels = [\"favorite_count\", \"retweet_count\"]\n","\n","  print(\"Repr√©sentation des nombres de retweets et de favoris de chaque tweet de {} par date\".format(userID))\n","  fig = plt.figure(figsize=(13,3))\n","  fig.subplots_adjust(hspace=0.01,wspace=0.01)\n","\n","  n_row = len(ylabels)\n","  n_col = 1\n","  for count, ylabel in enumerate(ylabels):\n","      ax = fig.add_subplot(n_row, n_col, count + 1)\n","      ax.plot(df_temp[\"created_at\"], df_temp[ylabel])\n","      ax.set_ylabel(ylabel)\n","  \n","  plt.show()"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":481,"status":"ok","timestamp":1644508190323,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"_EX8kGe5PMnX"},"outputs":[],"source":["visualize_count_favorites(df_tweets, \"JeanLuc_Melenchon\")\n","print(\"\\n\")\n","visualize_count_favorites(df_tweets, \"Marine_Lepen\")"]},{"cell_type":"markdown","metadata":{"id":"u0lKdMMwADR0"},"source":["> **Question** : Qu'observe-t'on ? "]},{"cell_type":"markdown","metadata":{"id":"VwudV8x6DFdE"},"source":["**R√©ponse** : TODO"]},{"cell_type":"markdown","metadata":{"id":"xBKjp28WTWCo"},"source":["##### Taille des tweets par politique \n","\n","Est-ce que des candidats font des tweets + ou - longs que d'autres ? "]},{"cell_type":"markdown","metadata":{"id":"x2GQA6vmBS8N"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> Utilisez la fonction <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html\" >.split()</a> pour calculer la taille des tweets</li>\n","    <li> Vous pouvez utiliser la fonction groupby et describe() </li>\n","\n","</ul>\n","</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_To_KRC63G6J"},"outputs":[],"source":["# Calcul d'une variable contenant le nombre de mots de chaque tweets\n","#TODO\n","\n","# Calcul de la distribution de la variable pour chaque politique\n","#TODO"]},{"cell_type":"markdown","metadata":{"id":"vdKXU6wpDV4c"},"source":["**R√©ponse** : TODO"]},{"cell_type":"markdown","metadata":{"id":"XqT4UpSJTfOH"},"source":["##### Lecture de quelques tweets"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1644507616477,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"5lQNubmtRSPo"},"outputs":[],"source":["def print_famous_tweets(userID, nb_favorites) :\n","\n","  ''' Cette fonction permet de s√©lectionner les tweets qui ont eu le plus de favoris \n","  pour un user_id donn√©, et de lire le tweet avec les indicateurs des autres variables de la \n","  base de donn√©es  \n","  '''\n","\n","  df_sub = df_tweets.loc[(df_tweets.user_id==userID) & (df_tweets.favorite_count > nb_favorites),:]\n","  for irow in range(df_sub.shape[0]):\n","      df_row = df_sub.iloc[irow,:]\n","    \n","      print(df_row[\"created_at\"])\n","      print(\"favorite_count={:6} retweet_count={:6}\".format(df_row[\"favorite_count\"],df_row[\"retweet_count\"]))\n","      print(colored(df_row[\"text\"], 'magenta'))\n","      print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPPcOCJ3Rjkj"},"outputs":[],"source":["print_famous_tweets(\"JeanLuc_Melenchon\", 20000)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":450,"status":"ok","timestamp":1644508299849,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"-5zzgYN4vEBe"},"outputs":[],"source":["print_famous_tweets(\"Marine_Lepen\", 10000)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":263,"status":"ok","timestamp":1644508305808,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"Zraxd1fQtWs4"},"outputs":[],"source":["print_famous_tweets(\"Eric_Zemmour\", 20000)"]},{"cell_type":"markdown","metadata":{"id":"3kZrXKcf5auz"},"source":["> **Question** : Qu'y-a't'il de particulier dans les tweets par rapport √† un texte normal ?"]},{"cell_type":"markdown","metadata":{"id":"uFb2KGuIDeDk"},"source":["**R√©ponse** : TODO"]},{"cell_type":"markdown","metadata":{"id":"VeNx3LdQtuAw"},"source":["### **Filtres**\n","\n","- Filtre sur la date pour ne prendre en compte que la campagne √©lectorale (d√©but septembre 2021)\n","- Filtre sur certains candidats pour que les traitements ne soient pas trop longs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_k9q9EotuVV"},"outputs":[],"source":["DATE_MIN = \"2021-09-01 00:00:00\"\n","\n","df_tweets = df_tweets.loc[df_tweets[\"created_at\"] >= datetime.datetime.strptime(DATE_MIN, \"%Y-%m-%d %H:%M:%S\")] \n","\n","print(f\"Taille du dataframe : {len(df_tweets)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMSBHP1d_O4C"},"outputs":[],"source":["candidats_select = [] #TODO : choisir des candidats\n","                    \n","df_tweets = df_tweets.loc[df_tweets.user_id.isin(candidats_select)]\n","\n","print(f\"Taille du dataframe : {len(df_tweets)}\")"]},{"cell_type":"markdown","metadata":{"id":"8DhFHdNx0cMZ"},"source":["## **3. Preprocessing du texte**\n","\n","On va prendre en compte les particularit√©s des tweets pour nettoyer le texte. \\\n","On va tester les techniques de preprocessing des cours pr√©c√©dents sur du texte fran√ßais : \n","- stopwords\n","- lemmatisation\n","- tokenisation\n"]},{"cell_type":"markdown","metadata":{"id":"JKly-cTpaO0x"},"source":["### Nettoyage du texte\n","Dans cette partie du TP, on nettoie le texte pour enlever les mots qui vont rajouter du bruit √† l'analyse (et ne rien apporter) \\\n","Pour nettoyer le texte : \n","- suppression des chiffres\n","- suppression de certaines expressions gr√¢ce √† des expressions r√©guli√®res\n","- suppression des stopwords\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXZ5nMoMGm_x"},"outputs":[],"source":["# on charge le mod√®le fran√ßais de spacy\n","nlp = fr_core_news_md.load()\n","print(len(nlp.Defaults.stop_words))\n","\n","# on peut rajouter des stopwords √† la liste de spacy de cette mani√®re : \n","nlp.Defaults.stop_words |= {\"avoir\", \"falloir\", \"faire\", \"monsieur\", \"direct\",\n","                            \"interview\", \"livetweet\", \"suivez\", r\"invit√©\\w+\", r\"(cha√Æne )?youtube\", \"mlp\"}\n","\n","# boucle pour que les stopwords ajout√©s fonctionnent\n","for word in nlp.Defaults.stop_words :\n","    for w in (word, word[0].capitalize(), word.upper()):\n","        lex = nlp.vocab[w]\n","        lex.is_stop = True\n","\n","# nombre de stopwords \n","len(nlp.Defaults.stop_words)"]},{"cell_type":"markdown","metadata":{"id":"Y1OFocC9d6cG"},"source":["> **Conseil** :  Regarder toujours la liste enti√®re de stopwords propos√©s pour enlever certains mots qui seraient utiles dans votre √©tude ou rajouter des stopwords non pr√©sents dans la liste"]},{"cell_type":"markdown","metadata":{"id":"3koKOPcshOBZ"},"source":["La cellule ci-dessous donne un exemple d'informations que peut donner Spacy : "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Oz6LXuV9r3T"},"outputs":[],"source":["doc = nlp(\"Demain je travaille \\\\n\\\\n √† la maison. #fatigu√© @hetik \\\\n https://test.com\")\n","\n","list_spacy = []\n","                \n","for token in doc : \n","  list_spacy.append([token.text,\n","                        token.idx,\n","                        token.lemma_,\n","                        token.is_punct,\n","                        token.is_space,\n","                        token.is_alpha,\n","                        token.shape_,\n","                        token.pos_,\n","                        token.tag_,\n","                        token.ent_type_])\n","  \n","exemple_spacy = pd.DataFrame(list_spacy, columns=[\"text\", \"idx\",\"lemma\",\"is_punct\",\"is_space\",\"is_alpha\",\"shape\",\"pos\",\"tag\",\"ent_type\"])\n","exemple_spacy"]},{"cell_type":"markdown","metadata":{"id":"Jmt4HyRYFc3v"},"source":["Expressions r√©guli√®res pour nettoyer le texte "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lRtXHvi8K_H"},"outputs":[],"source":["regexp_link = re.compile(r\"http\\S+\") # suppression des liens\n","regexp_number = re.compile(r\"\\d+[h., ]?\\d*\") # suppression des chiffres"]},{"cell_type":"markdown","metadata":{"id":"MAyZ8veVFz36"},"source":["**TODO** : Cr√©er une expression r√©guli√®re pour supprimer les hashtags et @ \n","\n","Remplacer le #TODO dans la cellule suivante par une expression r√©guli√®re.\n","\n","Votre regexp fonctionne si vous trouver \" √ßa  marche  !!\""]},{"cell_type":"markdown","metadata":{"id":"W9a6YQrBGs8s"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","Lorsque vous cherchez √† cr√©er des expressions r√©guli√®res, vous pouvez vous aider en allant sur ce site : <a href=\"https://regex101.com/\" >regex101.com</a> \n","</p> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHi1pQKqFzAv"},"outputs":[],"source":["# suppression des hashtags et @\n","regexp_hashtags = re.compile(r\"#TODO\")    \n","\n","test_hashtags = \"#Fuck √ßa #ne marche @pas !!\"\n","re.sub(regexp_hashtags, \"\", test_hashtags)"]},{"cell_type":"markdown","metadata":{"id":"6nxhbp6BtLeK"},"source":["Cr√©ation de la fonction de nettoyage du texte \n","\n","**TODO** : coder plusieurs fonctions :      \n","- une fonction `clean_text_spacy` qui prend en entr√©e un tweet et utilise spacy pour :     \n","    - supprimer les ponctuations ; \n","    - supprimer les stopwords ; \n","    - supprimer les caract√®res de type espace (/n, /t, etc.)\n","Cette fonction garde les tokens entiers\n","- une fonction `clean_lemmatize` :     \n","    - supprimer les ponctuations ; \n","    - supprimer les stopwords ; \n","    - supprimer les caract√®res de type espace (/n, /t, etc.)\n","Cette fonction garde non pas les tokens entiers, mais les lemmes. \n","- une fonction chapeau `preprocess_tweet` qui : \n","  - met les mots en minuscule\n","  - supprime les mots des expressions r√©guli√®res\n","  - au choix applique la fonction `clean_text_spacy` ou `clean_lemmatize`"]},{"cell_type":"markdown","metadata":{"id":"BuNS4dWLflUT"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","Lorsque vous utilisez les fonctions de spacy, vous allez potentiellement les tokeniser directement (et r√©cup√©rer une liste au lieu d'un texte). Pour √©viter cela, transformez le r√©sultat de cette mani√®re :    \n","\n","```\n","result = \" \".join(result)\n","```\n","\n","</p> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8dU2rFVIEvt"},"outputs":[],"source":["def clean_txt_spacy(doc):\n","  txt = #TODO\n","  return txt\n","\n","def clean_lemmatize(doc):\n","  lemmatized_txt = #TODO\n","  return lemmatized_txt\n"]},{"cell_type":"markdown","metadata":{"id":"bYG5VdneVdJB"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","- Utiliser re.sub() pour supprimer les liens, hashtags, chiffres\n","</p> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJYjHlY6uuTV"},"outputs":[],"source":["def preprocess_tweet(text, lemmatizing = True):\n","\n","  '''Fonction permettant de nettoyer le texte. Elle renvoie un string (pas de tokenisation encore)'''\n","  text_clean = text.lower().encode('utf-8').decode('utf-8')\n","  #TODO : supprimer du texte les liens, hashtags et chiffres avec les regexp pr√©c√©dentes\n","  text_clean = #TODO suppression liens\n","  text_clean = #TODO suppression hashtags\n","  text_clean = #TODO suppression chiffres\n","\n","  doc = nlp(text_clean)  \n","  if lemmatizing : \n","    preprocessed_tweet = clean_lemmatize(doc)\n","  else : \n","    preprocessed_tweet = clean_txt_spacy(doc)\n","\n","  return preprocessed_tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q29TpjgduNNn"},"outputs":[],"source":["# exemple pour tester sa fonction \n","tweet_test = \"Ils Pensaient se moquer #non, ils m'ont donn√© 1 slogan !üòÑ \\n\\n- Entretien √† d√©couvrir et partager \\n\\nhttps://t.co/Yn60Areagu\"\n","preprocess_tweet(tweet_test, lemmatizing=True)"]},{"cell_type":"markdown","metadata":{"id":"EM7IBszTuzPI"},"source":["R√©ponse attendue : \n","```python \n","'pensaient moquer donner slogan üòÑ entretien d√©couvrir partager'\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIm-BrUhur1c"},"outputs":[],"source":["# On peut alors nettoyer nos tweets, et cr√©er une nouvelle colonne, text_preprocess\n","# cela peut prendre un peu de temps √† tourner\n","df_tweets[\"text_preprocess\"] = df_tweets[\"text\"].apply(lambda tweet : preprocess_tweet(tweet, lemmatizing=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3r_igOqC6JGL"},"outputs":[],"source":["# On regarde le r√©sultat du nettoyage du texte\n","pd.set_option(\"max_colwidth\", None)\n","df_tweets[[\"text\", \"text_preprocess\"]].head(10)"]},{"cell_type":"markdown","metadata":{"id":"IDWBIN2KFsjq"},"source":["> Le preprocess n'est pas encore parfait, on pourrait enlever les verbes avec du pos-tagging ou bien rajouter l'info de pos-tagging apr√®s chaque mot. \\\n","> Supprimer les emojis ou les transformer en texte."]},{"cell_type":"markdown","metadata":{"id":"B2ksZmsUUvxg"},"source":["### Tokenisation\n","On tokenise la colonne de tweets pr√©trait√©s (preprocess)\n","\n","**TODO** : utiliser le module nltk pour tokeniser un tweet avec la fonction tokenisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fN1VVxSwqd2z"},"outputs":[],"source":["nltk.download('punkt') # n√©cessaire pour la tokenisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8hFhNcRyQs7"},"outputs":[],"source":["# Remplir le None dans le code\n","def tokenisation(tweet):\n","  tweet_tokenized = #TODO\n","  return(tweet_tokenized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJYs9NjVs7G9"},"outputs":[],"source":["df_tweets[\"tokens\"] = df_tweets[\"text_preprocess\"].apply(lambda tweet : tokenisation(tweet))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8_14mLZtL2Q"},"outputs":[],"source":["df_tweets[[\"text_preprocess\", \"tokens\"]].head()"]},{"cell_type":"markdown","metadata":{"id":"c9Ej6e6-08cE"},"source":["### Analyse du preprocess\n","\n","On regarde un peu les r√©sultats du preprocessing : \n","- combien y a-t-il de mots distincts pour chacun des deux hommes politiques ? \n","- Quels sont les mots les plus utilis√©s par deux candidats de votre choix ? \n","\n","Pour cela vous vous aiderez des deux fonctions donn√©es ci-dessous"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4N6HGJWOHcW"},"outputs":[],"source":["def create_big_tweet_by_userid(userid, col_text) : \n","\n","  ''' Fonction pour mettre tous les tweets de chaque politiciens dans un m√™me text (string) '''\n","  one_big_tweet = \" \".join(df_tweets.loc[df_tweets[\"user_id\"] == userid, col_text])\n","  \n","  return one_big_tweet\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"711wc14pWEnN"},"outputs":[],"source":["def get_n_most_common_words(list_words, n) :\n","\n","  ''' Fonction permettant de donner les n mots les plus fr√©quents d'une liste de mots '''\n","  freq_words = nltk.FreqDist(list_words)\n","  print(freq_words.most_common(n))\n"]},{"cell_type":"markdown","metadata":{"id":"5UlRdmV2XAdH"},"source":["**TODO** : Si on n'utilise pas de preprocessing, quels sont les mots les plus utilis√©s par les 2 politiciens ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CY-y0Vy5VwAi"},"outputs":[],"source":["# Cr√©er un gros tweet pour chacun des deux politiques (qui est la jointure de l'ensemble de ses tweets)\n","big_tweet_candidate1 = #TODO\n","big_tweet_candidate2 = #TODO\n","\n","# Tokeniser le gros tweet de chacun des politiques\n","tokens_candidate1 = #TODO\n","tokens_candidate2 = #TODO\n","\n","# Regarder les 10 mots les plus communs pour chacun des politiques\n","#TODO pour chaque candidat"]},{"cell_type":"markdown","metadata":{"id":"wnoa-PD0Wt-f"},"source":["**R√©ponse** : "]},{"cell_type":"markdown","metadata":{"id":"GLwisYcq12sG"},"source":["Sans preprocessing, combien y a-t-il de mots distincts pour chaque politique ?"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1644508165793,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"RSof4c_fX5pm"},"outputs":[],"source":["# la fonction set appliqu√©e sur une liste donne une liste d'√©l√©ments uniques\n","print(\"Nombre de mots distincts dans les tweets du candidat 1 : {}\".format(len(set(tokens_candidate1))))\n","print(\"Nombre de mots distincts dans les tweets du candidat 2 : {}\".format(len(set(tokens_candidate2))))"]},{"cell_type":"markdown","metadata":{"id":"63-7MDKW2dVj"},"source":["**R√©ponse** : \n","\n","Jean Luc M√©lenchon : 11877 \\\n","Eric Zemmour : 8960 \\\n","Marine Lepen : 8108 \\\n","Emmanuel Macron : 4394"]},{"cell_type":"markdown","metadata":{"id":"4DIHPg2YXLy-"},"source":["**TODO** : m√™me question avec un preprocessing ?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNzWJEz36JMI"},"outputs":[],"source":["# Cr√©er un gros tweet pour chacun des deux politiques (qui est la jointure de l'ensemble de ses tweets)\n","big_tweet_candidate1 = #TODO\n","big_tweet_candidate2 = #TODO\n","\n","# Tokeniser le gros tweet de chacun des politiques\n","tokens_candidate1 = #TODO\n","tokens_candidate2 = #TODO\n","\n","# Regarder les 10 mots les plus communs pour chacun des politiques\n","#TODO pour chaque candidat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vP0H0WW22G1K"},"outputs":[],"source":["print(\"Nombre de mots distincts dans les tweets du candidat 1 : {}\".format(len(set(tokens_candidate1))))\n","print(\"Nombre de mots distincts dans les tweets du candidat 2 : {}\".format(len(set(tokens_candidate2))))"]},{"cell_type":"markdown","metadata":{"id":"JGzpTmsM8yAE"},"source":["**R√©ponse** : \n","\n","Jean Luc M√©lenchon : 5369 \\\n","Eric Zemmour : 4628"]},{"cell_type":"markdown","metadata":{"id":"tF4dqxlPnXcy"},"source":["### Nuage de mots\n","\n","On trace un nuage de mots pour chacun des politiques pour voir ce qui ressort"]},{"cell_type":"markdown","metadata":{"id":"jAVwmN5IvBbD"},"source":["**#TODO** : Faire un nuage de mots pour deux candidats de votre choix avec 30 mots"]},{"cell_type":"markdown","metadata":{"id":"1usW-d1wvLbl"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> transformer l'ensemble des tweets d'un politique en un texte unique </li>\n","    <li> vous pouvez utiliser la fonction WordCloud </li>\n","</ul>\n","</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1644438018501,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"C12i7alUnWa_","outputId":"c2c1ba65-76aa-4a3a-820c-d852d2aeb18a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wordcloud des mots lemmatis√©s de l'ensemble des tweets du candidat 1\n"]}],"source":["# Faire un texte unique pour les tweets de JLM \n","#TODO\n","print(\"Wordcloud des mots lemmatis√©s de l'ensemble des tweets du candidat 1\")\n","#Faire le wordcloud \n","#TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644438018504,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"aogPxGLjoYG1","outputId":"991d6297-3872-4c5f-9392-95e8d946700d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wordcloud des mots lemmatis√©s de l'ensemble des tweets du candidat 2\n"]}],"source":["# Faire un texte unique pour les tweets de JLM \n","#TODO\n","print(\"Wordcloud des mots lemmatis√©s de l'ensemble des tweets du candidat 2\")\n","#Faire le wordcloud \n","#TODO"]},{"cell_type":"markdown","metadata":{"id":"GuxgMTF3Oo4T"},"source":["C'est bien beau, mais c'est difficile √† analyser, et surtout √† comparer... \\\n","On va utiliser scattertext pour comparer r√©ellement le vocabulaire des 2 politiques."]},{"cell_type":"markdown","metadata":{"id":"wIIsh5K3iSnx"},"source":["## **4. Scattertext**\n","\n","Gr√¢ce √† Scattertext, on va pouvoir comparer de mani√®re visuelle la distinction de vocabulaire utilis√© par deux candidats de votre choix. \n"]},{"cell_type":"markdown","metadata":{"id":"0TE5QCkLHvDb"},"source":["On doit d'abord construire un corpus avec nos donn√©es : \n","- donner la variable de cat√©gorie \n","- donner la variable du texte\n","\n","On peut rajouter le partie ```.compact(st.AssociationCompactor(4000))``` pour ne prendre en compte que les 4000 mots les plus importants dans le scattertext.\n","\n","**TODO** : cr√©er le corpus avec la fonction donn√©e ci-dessous"]},{"cell_type":"markdown","metadata":{"id":"LkUrf23zwB5_"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> Filtrer en gardant les tweets des deux candidats de votre choix </li>\n","</ul>\n","</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CPhJNikb6Wp"},"outputs":[],"source":["df_sample = df_tweets.loc[df_tweets.user_id.isin([#TODO : candidats de votre choix])]\n","\n","# on cr√©e un objet corpus pour scattertext\n","corpus = st.CorpusFromPandas(data_frame = df_sample,\n","                             category_col = , #TODO\n","                             text_col = , #TODO\n","                             nlp = nlp).build().compact(st.AssociationCompactor(4000))"]},{"cell_type":"markdown","metadata":{"id":"QpCPtMrId1zV"},"source":["Une fois le corpus cr√©√©, on peut cr√©er le html avec le scattertext. \n","\n","On utilise la fonction ```st.produce_scattertext_explorer``` en donnant les param√®tres vus pendant le cours : \n","- term_ranker\n","- term_scorer\n","- transform \n","\n","**TODO** : remplir la fonction en r√©fl√©chissant aux param√®tres que vous voulez tester."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3075,"status":"ok","timestamp":1612125176049,"user":{"displayName":"Manon Richard","photoUrl":"","userId":"05405874535388750771"},"user_tz":-60},"id":"-RWsqgPatg5v","outputId":"3d753b97-f7ac-4d16-dbfe-41aa8380273f"},"outputs":[{"data":{"text/plain":["1449245"]},"execution_count":127,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# On cr√©e le html du scattertext\n","html = st.produce_scattertext_explorer(  corpus\n","                                       , category                  = #TODO\n","                                       , category_name             = #TODO\n","                                       , not_category_name         = #TODO\n","                                       , minimum_term_frequency    = #TODO\n","                                       , pmi_threshold_coefficient = 1\n","                                       , term_ranker               = #TODO\n","                                       , term_scorer               = #TODO\n","                                       , transform                 = #TODO\n","                                       , width_in_pixels           = 1000\n","                                       )\n","\n","# On enregistre le html\n","open(\"tweets_visualisation.html\", 'wb').write(html.encode('utf-8'))"]},{"cell_type":"markdown","metadata":{"id":"FqieNcsrJ7DH"},"source":["**TODO** : regarder le r√©sultat (il apparaitra normalement dans le dossier content du notebook) en t√©l√©chargeant le html (cela peut prendre un petit moment avant de s'afficher correctement).\n","\n","**Question** : Analysez le graphique"]},{"cell_type":"markdown","metadata":{"id":"2KA3He08v2in"},"source":["## **5. Mod√©lisation**\n","\n","On souhaite pr√©dire si un tweet provient du compte de Marine Le Pen, de Jean Luc M√©lenchon, d'Eric Zemmour ou d'Emmanuel Macron. Pour cela, on a besoin de : \n","- Cr√©er un √©chantillon train / dev\n","- pr√©parer le text (pr√©processing)\n","- cr√©er des features (plusieurs m√©thodes : bag of words, counts of words, etc.)\n","- r√©aliser l'algorithme\n","- √©valuer la performance du mod√®le "]},{"cell_type":"markdown","metadata":{"id":"54J4-pgJqsEa"},"source":["### Cr√©ation des √©chantillons \n","\n","**TODO** : cr√©er un √©chantillon train (70% du jeu de donn√©es total) et un √©chantillon test "]},{"cell_type":"markdown","metadata":{"id":"8YO7cfe6QT9f"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","<ul>\n","  <li> Utilisez la fonction <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" >train_test_split</a></li>\n","  <li> Vous pouvez prendre tout le df_tweet dans le X</li>\n","  <li> N'oubliez pas de rajouter un random_seed pour avoir des r√©sultats reproductibles</li>\n","</p> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCnSMLCMqrX7"},"outputs":[],"source":["df_train, df_test, y_train, y_test = # None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3cTibPARhp5"},"outputs":[],"source":["print(f\"Nombre de tweets dans l'√©chantillon train : {len(df_train)}\")\n","print(f\"Nombre de tweets dans l'√©chantillon test : {len(df_test)}\")"]},{"cell_type":"markdown","metadata":{"id":"bWusYW-sRiqz"},"source":["**R√©ponse** : \n","\n","Nombre de tweets dans l'√©chantillon train : 4449 \\\n","Nombre de tweets dans l'√©chantillon test : 1907"]},{"cell_type":"markdown","metadata":{"id":"xhTsIINSumdB"},"source":["### Mod√®le de r√©gression multinomiale sans gridsearch \n","\n","- Transformer le texte de df_train et df_test en vecteurs pour le mod√®le\n","- Utiliser la r√©gression logistique multinomiale sans param√®tre\n","- Regarder les param√®tres s√©lectionn√©s\n","- Regarder le score sur l'√©chantillon test\n","\n","**#TODO** : transformer df_train pour que ce ne soit plus des tweets, mais des vecteurs gr√¢ce √† <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" >TfidfVectorizer</a> "]},{"cell_type":"markdown","metadata":{"id":"LmtC2j6qTIGP"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","\n","La fonction TfidfVectorizer a des param√®tres que vous pouvez choisir : \n","<ul>\n","\n","- Combien de n-grams : vous consid√©rez mot par mot ou bien √©galement des groupes de 2 mots\n","- max_df : si vous voulez enlever un pourcentage de mots les plus fr√©quents\n","- min_df : si vous voulez enlever un pourcentage de mots les moins fr√©quents\n","</p> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwDLrodDuni9"},"outputs":[],"source":["#TODO"]},{"cell_type":"markdown","metadata":{"id":"n7VV9vWNuq_9"},"source":["**#TODO** : cr√©er le mod√®le de r√©gression logistique (OVR) et entrainer le mod√®le sur les donn√©es d'apprentissage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Px7lwclyurWP"},"outputs":[],"source":["# initialiser le mod√®le \n","model = #TODO\n","\n","# entrainer le mod√®le avec les donn√©es d'apprentissage\n","model_default_fit = #TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXcPasObu_3X"},"outputs":[],"source":["# vous pouvez voir les param√®tres du mod√®le \n","model_default_fit.get_params(deep=True)"]},{"cell_type":"markdown","metadata":{"id":"ufIHpn-0vN4F"},"source":["**#TODO** : Regarder la performance du mod√®le sur l'√©chantillon train et test (accuracy)"]},{"cell_type":"markdown","metadata":{"id":"nT0QdtanvcF1"},"source":["<details>    \n","<summary>\n","    <font size=\"3\"**texte en gras** color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","<ul>\n","  <li> N'oubliez pas de transformer le texte de l'√©chantillon test en vecteur en amont</li>\n","  <li> Vous pouvez utiliser la fonction <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">score</a>  du mod√®le</li>\n","  \n","</p> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlJ3l5nzvXyL"},"outputs":[],"source":["#TODO\n","X_test = vectorizer.transform(X_test)\n","model_default_fit.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"LLc6TpsmwNLj"},"source":["**Question** : Quel est l'accuracy sur l'√©chantillon test ? \n","\n","**R√©sultat** : **#TODO**"]},{"cell_type":"markdown","metadata":{"id":"TUzF3ZICDJI2"},"source":["**#TODO** : regarder plus en d√©tail ce que donne le mod√®le : "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AoZG1X5DbTe"},"outputs":[],"source":["print(\"le 1er tweet de l'√©chantillon test a √©t√© pr√©dit : \")\n","print(model_default_fit.predict(X_test[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehLcMklODeZG"},"outputs":[],"source":["model_default_fit.predict_proba(X_test[0])"]},{"cell_type":"markdown","metadata":{"id":"gRcESsThDgVj"},"source":["**Question** : que retourne la ligne de code ci-dessus ?\n","\n","**R√©ponse** : "]},{"cell_type":"markdown","metadata":{"id":"b-MAFb_90PQ5"},"source":["### Mise en place de la RandomSearch\n","\n","On veut mettre en place une randomSearch pour s√©lectionner les meilleurs param√®tres qu'on a choisi d'√©valuer via la m√©thode de cross-validation : \n","- on √©tablit d'abord la grille de param√®tres que l'on veut tester\n","- on effectue la RandomSearch\n","- on regarde les r√©sultats sut l'√©chantillon test"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":635,"status":"ok","timestamp":1644503691304,"user":{"displayName":"Manon Richard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05405874535388750771"},"user_tz":-60},"id":"-c7KHH7C0VzW"},"outputs":[],"source":["# Param√®tres √† tester donn√©es, vous pouvez modifier pour tester d'autres param√®tres\n","dict_params = dict(prep__text_preprocess__max_df=[0.99, 0.95, 0.9],\n","                   prep__text_preprocess__min_df=[2, 5, 10],\n","                   clf__C = [1, 20, 50],\n","                   clf__penalty = ['l2'],\n","                   clf__multi_class=['ovr', 'multinomial'])"]},{"cell_type":"markdown","metadata":{"id":"bBmjbxp-12cz"},"source":["**#TODO** : Entrainer la Randomsearch avec les param√®tres ci-dessus sur les donn√©es d'apprentissage avec de la cross validation\n","\n","NB : une pipeline a √©t√© mise en place dans la cellule ci-dessous afin de pouvoir tester √©galement les param√®tres du TFIDF. "]},{"cell_type":"markdown","metadata":{"id":"txvwXcCG2AZz"},"source":["<details>    \n","<summary>\n","    <font size=\"3\"**texte en gras** color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","<ul>\n","  <li> Vous pouvez utiliser la fonction <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">RandomizedSearchCV</a></li>\n","  \n","</p> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuVygAck1hfy"},"outputs":[],"source":["# Entrainer le randomizedsearch \n","\n","# Vectorisation de la variable text_preprocess\n","text_transformer_tfidf =  TfidfVectorizer()\n","preprocess = ColumnTransformer([(\"text_preprocess\", text_transformer_tfidf, \"text_preprocess\")], \n","                               remainder=\"drop\")\n","\n","# Type de mod√®le √† tester\n","model = LogisticRegression(random_state=54269, max_iter=1000)\n","\n","# Pipeline qui combine le preprocess et le mod√®le\n","prep_model = Pipeline(steps=[('prep',preprocess),\n","                             ('clf', model)])\n","\n","# RANDOMIZED SEARCH\n","##### A MODIFIER A PARTIR D'ICI #####\n","random_search = RandomizedSearchCV()#TODO\n","### FIN MODIFICATION ### \n","\n","best_rd_model = random_search.fit(df_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeH683TWrq3p"},"outputs":[],"source":["# Meilleurs param√®tres s√©lectionn√©s par la randomSearch\n","best_rd_model.best_estimator_"]},{"cell_type":"markdown","metadata":{"id":"ZCxF5QnBEVN-"},"source":["Quelle accuracy le meilleur mod√®le a-t'il atteint sur l'√©chantillon train ? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhIH5Q-nEOau"},"outputs":[],"source":["# R√©sultats du meilleur mod√®le sur l'√©chantillon train \n","best_rd_model.best_score_"]},{"cell_type":"markdown","metadata":{"id":"XsrEVM_BDSsv"},"source":["**#TODO** : on √©value la performance en calculant l'accuracy du mod√®le s√©lectionn√© par randomsearch sur l'√©chantillon test\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGAlWY4MDU5T"},"outputs":[],"source":["grid_search.score(#TODO)"]},{"cell_type":"markdown","metadata":{"id":"fhjN7HYwAoEY"},"source":["### Evaluation de la performance du mod√®le \n","\n","On va calculer la matrice de confusion sur l'√©chantillon test"]},{"cell_type":"markdown","metadata":{"id":"XeLQfbK8CQ_R"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n","</summary>\n","<p>\n","<ul>\n","  <li> Pr√©dire les candidats de df_test dans un premier temps </li>\n","  <li> Vous pouvez utiliser la fonction <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\" >ConfusionMatrixDisplay</a></li>\n","  \n","</p> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ld_5KZH0nBdC"},"outputs":[],"source":["#matrice de confusion\n","#confrontation entre Y observ√© sur l‚Äô√©ch. test et la pr√©diction\n","#TODO"]},{"cell_type":"markdown","metadata":{"id":"rj3VCS6pCjid"},"source":["**#TODO** : Interpr√©ter les r√©sultats "]},{"cell_type":"markdown","metadata":{"id":"p6mh9hTfD5Hm"},"source":["**Question** : Sur l'ensemble des tweets de JLM, combien (pourcentage) ont bien √©t√© pr√©dits JLM ?  "]},{"cell_type":"markdown","metadata":{"id":"ZU3BezsnD-oo"},"source":["**R√©ponse** :"]},{"cell_type":"markdown","metadata":{"id":"xWLcvP5n23bF"},"source":["### Test sur des nouvelles donn√©es :\n","\n","Ces quelques tweets ont √©t√© r√©cup√©r√©s apr√®s que la base de donn√©es ait √©t√© r√©cup√©r√©e. Ce sont donc des nouvelles donn√©es que le mod√®le n'a jamais vu.\n","\n","**#TODO** : Qui a publi√© ces tweets ? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzlLcVIf6-st"},"outputs":[],"source":["# Visualisation des tweets √† pr√©dire\n","df_mystere = pd.read_excel(f\"{PATH_DATA}/test_mystere.xlsx\")\n","df_mystere[\"text\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjIzQri67kcc"},"outputs":[],"source":["# On pr√©pare les donn√©es pour que df_mystere ait la m√™me structure que df_train\n","df_mystere[\"text_preprocess\"] = df_mystere.text.apply(lambda row : preprocess_tweet(row, lemmatizing=True))\n","df_mystere[\"tokens\"] = df_mystere.text_preprocess.apply(lambda row : tokenisation(row))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUwrO9j5RaEG"},"outputs":[],"source":["# R√©aliser la pr√©diction avec l'un des deux mod√®les r√©alis√©s\n","#TODO "]},{"cell_type":"markdown","metadata":{"id":"M9bYUtGm7YwQ"},"source":["**R√©ponse attendue** : \n","\n","```\n","array(['Eric_Zemmour', 'Eric_Zemmour', 'JeanLuc_Melenchon',\n","       'Emmanuel_Macron', 'JeanLuc_Melenchon'], dtype=object)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbbWPGl9ZCLK"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO2ZhNiS1ymBY7keChedWuv","collapsed_sections":[],"mount_file_id":"1AAO-yWTOYd02mnsXe0ymYAioSj814bfC","name":"TP_analyse_tweets_TODO.ipynb","provenance":[{"file_id":"10pbABCw48Nxk7iFpAxzvSvZ7b1EiUP3h","timestamp":1612001150449}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
